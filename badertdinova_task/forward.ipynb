{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "from model import DNN\n",
    "from forward_config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PINN_nonlinear_flow\\Inverse_task\\badertdinova_task\\forward.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m<\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m         train(epoch, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         train(epoch, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32md:\\PINN_nonlinear_flow\\Inverse_task\\badertdinova_task\\forward.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Optimize loss function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m loss_pde \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m3000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\lizaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lizaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lizaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\lizaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 143\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    146\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32md:\\PINN_nonlinear_flow\\Inverse_task\\badertdinova_task\\forward.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     L_t, W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss_weights(grid)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     loss_bnd1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_dirichlet(bnd1, bndval1)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PINN_nonlinear_flow/Inverse_task/badertdinova_task/forward.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     loss_bnd2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_dirichlet(bnd2, bndval2)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\PINN_nonlinear_flow\\Inverse_task\\badertdinova_task\\model.py:41\u001b[0m, in \u001b[0;36mDNN.loss_weights\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_weights\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_pde(x)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_t, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m     L_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(op\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_t, \u001b[39m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32md:\\PINN_nonlinear_flow\\Inverse_task\\badertdinova_task\\model.py:32\u001b[0m, in \u001b[0;36mDNN.loss_pde\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m p_x, p_t \u001b[39m=\u001b[39m dp_g[:, \u001b[39m1\u001b[39m:], dp_g[:, \u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m p1 \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\u001b[39m/\u001b[39mmu_oil(p_x, x[:,\u001b[39m1\u001b[39m:])\u001b[39m*\u001b[39mp_x\n\u001b[1;32m---> 32\u001b[0m d2p_g, \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(p1\u001b[39m.\u001b[39;49msum(\u001b[39m0\u001b[39;49m), x, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     34\u001b[0m p_xx, _ \u001b[39m=\u001b[39m d2p_g[:, \u001b[39m1\u001b[39m:], d2p_g[:, \u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\n\u001b[0;32m     36\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx[:,\u001b[39m1\u001b[39m:])\u001b[39m*\u001b[39mr0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mbetta\u001b[39m/\u001b[39mt_end\u001b[39m/\u001b[39mk\u001b[39m*\u001b[39mp_t \u001b[39m-\u001b[39m p_xx\n",
      "File \u001b[1;32mc:\\Users\\lizaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    390\u001b[0m     result \u001b[39m=\u001b[39m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(\n\u001b[0;32m    391\u001b[0m         grad_outputs_\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 394\u001b[0m     result \u001b[39m=\u001b[39m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    395\u001b[0m         t_outputs,\n\u001b[0;32m    396\u001b[0m         grad_outputs_,\n\u001b[0;32m    397\u001b[0m         retain_graph,\n\u001b[0;32m    398\u001b[0m         create_graph,\n\u001b[0;32m    399\u001b[0m         t_inputs,\n\u001b[0;32m    400\u001b[0m         allow_unused,\n\u001b[0;32m    401\u001b[0m         accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    402\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[39mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    404\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    405\u001b[0m         output\n\u001b[0;32m    406\u001b[0m         \u001b[39mif\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mzeros_like(\u001b[39minput\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    408\u001b[0m         \u001b[39mfor\u001b[39;00m (output, \u001b[39minput\u001b[39m) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(result, t_inputs)\n\u001b[0;32m    409\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_p_drop = []\n",
    "\n",
    "device = torch.device(device)\n",
    "lr = 0.001\n",
    "epochs = 24000\n",
    "\n",
    "layers = [200, 200, 200, 200, 1]\n",
    "\n",
    "L = 5.\n",
    "M = 5\n",
    "tol = 0\n",
    "n_t = 100\n",
    "\n",
    "u = torch.linspace(Rk/2/r0, Rk/r0, 70)\n",
    "u = torch.log(u)\n",
    "\n",
    "u1 = torch.linspace(np.log(rc/r0), np.log(Rk/2/r0), 50)\n",
    "u = torch.hstack((u, u1[:-1]))\n",
    "u = torch.sort(u)[0]\n",
    "\n",
    "t = torch.linspace(t0, 1, n_t)\n",
    "\n",
    "grid = torch.cartesian_prod(t, u).float().to(device)\n",
    "grid = grid[grid[:, 0].argsort()]\n",
    "\n",
    "# p(u,0)=1\n",
    "bnd1 = torch.cartesian_prod(torch.tensor([0.]), u).float().to(device)\n",
    "bndval1 = torch.tensor([1.]).to(device)\n",
    "\n",
    "#bnd2(u(-1), t)=1\n",
    "bnd2 = torch.cartesian_prod(t, torch.tensor([u[-1].item()])).float().to(device)\n",
    "bndval2 = torch.tensor([1.]).to(device)\n",
    "\n",
    "#bnd3\n",
    "bnd3 = torch.cartesian_prod(t, torch.tensor([u[0].item()])).float().to(device)\n",
    "\n",
    "\n",
    "#NN = Modified_MLP(layers, L, M, device='cuda')\n",
    "NN = nn.Sequential(\n",
    "    nn.Linear(2, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 1)).to(device)\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = DNN(NN, tol, n_t).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train PINNs\n",
    "def train(epoch, m):\n",
    "    model.train()\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        L_t, W = model.loss_weights(grid)\n",
    "        loss_bnd1 = model.loss_dirichlet(bnd1, bndval1).reshape(-1)\n",
    "        loss_bnd2 = model.loss_dirichlet(bnd2, bndval2).reshape(-1)\n",
    "        loss_bnd3 = model.loss_operator(bnd3).reshape(-1)\n",
    "        loss = torch.mean(W*L_t) + 10000*(torch.hstack([loss_bnd1, loss_bnd2, loss_bnd3])).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # Optimize loss function\n",
    "    loss_pde = optimizer.step(closure)\n",
    "    if epoch % 3000 == 0:\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_last_lr())\n",
    "    loss_value = loss_pde.item() if not isinstance(loss_pde, float) else loss_pde\n",
    "\n",
    "\n",
    "    if epoch%500==0:\n",
    "        print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "        fig1 = plt.figure()\n",
    "        ax1 = fig1.add_subplot(projection='3d')\n",
    "        ax1.plot_trisurf(grid[:, 1].cpu().detach().numpy().reshape(-1), grid[:, 0].cpu().detach().numpy().reshape(-1),\n",
    "                    model(grid).cpu().detach().numpy().reshape(-1), cmap=cm.jet, linewidth=0.2, alpha=1)\n",
    "        ax1.set_xlabel(\"x1\")\n",
    "        ax1.set_ylabel(\"x2\")\n",
    "        plt.show()\n",
    "\n",
    "# Print CPU\n",
    "print('Start training...')\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    if epoch < 200:\n",
    "        train(epoch, None)\n",
    "    else:\n",
    "        train(epoch, 1)\n",
    "\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
